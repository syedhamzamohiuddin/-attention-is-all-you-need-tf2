# ğŸ§  Transformer: Attention Is All You Need â€” A Faithful TensorFlow Implementation

This repository contains a **from-scratch, TensorFlow 2 implementation** of the Transformer architecture described in the paper [*Attention Is All You Need* (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762).  
  
It replicates the architecture and training pipeline in detail â€” including **three-way weight tying**, **causal masking**, and **original Moses tokenization** â€” and is trained on the **WMT14 Englishâ€“German** machine translation task.

---

## ğŸ“Œ Links

- ğŸ“˜ **Kaggle Notebook**:  
  [Transformer â€” Attention Is All You Need (Detailed)](https://www.kaggle.com/code/hamzamohiuddin/transformer-attention-is-all-you-need-detailed)  
  > ğŸ“Œ Written as a tutorial for those implementing the paper â€” includes detailed commentary, diagrams, and code explanations with references to paper sections.

- âœï¸ **Medium Article**:  
  [Implementing Attention Is All You Need â€” Lessons from Recreating the Transformer](https://medium.com/p/bada14b0023a)

---

## ğŸš€ Key Features

- âœ… **TensorFlow 2 implementation** of *Attention Is All You Need* â€” no shortcuts or black boxes  
- âœ… **Trained on WMT14 Englishâ€“German** using official scripts and preprocessed corpus  
- âœ… **Faithful preprocessing**: Moses tokenization + BPE (like the original paper)  
- âœ… **Accurate causal masking**, validated across attention, logits, and loss layers  
- âœ… **Three-way weight tying**: encoder embeddings, decoder embeddings, and output projection **share weights**  
  âš ï¸ *Often skipped in popular implementations â€” this repo preserves it as per the original paper*  
- âœ… **Complete training pipeline**: learning rate schedule, loss masking, label smoothing  
- âœ… **Visual explanations**: inline diagrams for masking, attention, positional encoding, and more  
- âœ… **End-to-end tested**: outputs verified at every stage â€” embeddings â†’ attention â†’ logits  
- âœ… **Educational structure**: built to teach paper implementation, not just replicate results

---

## ğŸ“· Visuals from the Notebook

<p align="center">
  <img src="https://storage.googleapis.com/kaggle-media/transformer_diagram.png" alt="Transformer Diagram" width="600"/>
  <br>
  <i>Detailed diagrams and tensor flows included to explain architecture components</i>
</p>

---

## ğŸ“„ Whatâ€™s Inside

This repo links to the full implementation and tutorial-style notebook:

| Component | Status |
|----------|--------|
| Encoder & Decoder Blocks | âœ… Fully implemented |
| Multi-Head Attention | âœ… Custom with masking support |
| Positional Encoding | âœ… Reimplemented with visualization |
| Embedding Layer | âœ… With shared weights (tied) |
| Training Schedule | âœ… Custom LR + label smoothing |
| Masking Logic | âœ… Causal & padding masks validated |
| Dataset Preprocessing | âœ… WMT14 + Moses/BPE |
| TensorFlow Code | âœ… Native TF2 without shortcuts |

---

## ğŸ“š Learning Outcomes

This project was created to:
- ğŸ§  **Understand every detail of the Transformer paper**
- ğŸ§ª **Answer real questions** about masking, autoregression, positional encoding, etc.
- ğŸ¯ **Train the full model** from scratch using open WMT14 data
- ğŸ› ï¸ **Recreate core components** without depending on libraries like HuggingFace or T2T
- âœ… **Validate implementation correctness** at every stage

---

## ğŸ§ª Why This Implementation Stands Out

- Most open-source Transformer repos skip key aspects like:
  - ğŸ” **Three-way weight tying**
  - ğŸ§± **Faithful masking logic**
  - ğŸ§¹ **Real preprocessing from WMT14 scripts**
- This implementation takes the paper **literally and rigorously**, as a learning and engineering challenge
- Ideal for:
  - Engineers studying transformers deeply
  - Students learning paper-to-code skills
  - Researchers verifying correctness of model components

---

## ğŸ§  Author

**Hamza Mohiuddin**  
Deep Learning Engineer â€” Computer Vision, Real-Time Systems, and Edge AI  
ğŸ“ *Always learning, always building*  
[LinkedIn](https://www.linkedin.com/in/hamzamohiuddin) â€¢ [Kaggle](https://www.kaggle.com/hamzamohiuddin) â€¢ [Medium](https://medium.com/@hamzamohiuddin)

---

## ğŸ“œ License

Released under the **MIT License** â€” free for personal and commercial use with attribution.

---

## ğŸ” GitHub Topics

